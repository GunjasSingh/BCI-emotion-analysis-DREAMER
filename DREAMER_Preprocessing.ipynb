{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DREAMER Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlvDMRRI/bX7ppyO8c28+B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSgleo8xSp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "33dfb7e9-df5a-4765-e84c-0d1ec8da19fe"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import svm\n",
        "from scipy import signal\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "\n",
        "!pip install neurokit2\n",
        "import neurokit2 as nk\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "!pip install mne\n",
        "import mne"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neurokit2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/d5/ed0003efbb2b84a57ccf6f351aa5447078a1829886945a42bf2b628c1f91/neurokit2-0.0.40-py2.py3-none-any.whl (980kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from neurokit2) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neurokit2) (1.0.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from neurokit2) (0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from neurokit2) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from neurokit2) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->neurokit2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neurokit2) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->neurokit2) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit2) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->neurokit2) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->neurokit2) (0.16.0)\n",
            "Installing collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.0.40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/ab/9b79f927b599da515335afb4b666a7bb336930a6d8345e7b483a9980a9c1/mne-0.20.7-py3-none-any.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.20.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK8IW_yt2GHi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "812d5f58-6336-49cf-dfb1-06e9f0d5da0a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_J6l62p1D3e"
      },
      "source": [
        "path = \"/content/drive/My Drive/DREAMERedit.mat\"\n",
        "raw = sio.loadmat(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IX8dPUbyrSJ"
      },
      "source": [
        "#Step 1 : frames\n",
        "#from mne.time_frequency import psd_welch\n",
        "\n",
        "def preprocess_EEG(raw, feature):\n",
        "\n",
        "  frame = [[0 for x in range(640)] for y in range(12)] \n",
        "\n",
        "  #Dividing into frames\n",
        "  for i in range(0,7680):\n",
        "    j = i//640\n",
        "    k = i%640\n",
        "    frame[j][k] = raw[i]\n",
        "  \n",
        "  #frame[0][0-639] - Frame 1, frame[11][0-639] - Frame 12\n",
        "  \n",
        "  samplingRate = 128 \n",
        "  nyquest = samplingRate/2.0\n",
        "  n=213\n",
        "  \n",
        "  # delta 0-4 Hz Band Filter\n",
        "  f = 4/nyquest\n",
        "  f1 = 40/nyquest\n",
        "  \n",
        "  fdelta=[]\n",
        "  pdelta=[]\n",
        "\n",
        "  fnew = [[0 for x in range(640)] for y in range(18)]\n",
        "  pnew = [[0 for x in range(640)] for y in range(18)]\n",
        "  thetamean =[]\n",
        "  alphamean =[]\n",
        "  betamean  =[]\n",
        "  gammamean =[]\n",
        "  deltamean =[]\n",
        "\n",
        "  delta_filter = signal.firwin(n, f, pass_zero=True, window ='hamming')\n",
        "  overall_filter = signal.firwin(n, f1, pass_zero=True, window=\"hamming\") #will be similar to orgiginal data\n",
        "\n",
        "  for i in range(0,12):\n",
        "\n",
        "    filt_overall = signal.filtfilt(overall_filter, 1, frame[i], method ='pad')\n",
        "    filt_delta = signal.filtfilt(delta_filter, 1, filt_overall)\n",
        "    f,p = signal.welch(filt_overall,fs =128, nperseg=256)\n",
        "    \n",
        "    fnew[i] = f\n",
        "    pnew[i] = p\n",
        "    \n",
        "    #plt.semilogy(fnew[i], pnew[i])\n",
        "    #plt.xlabel('frequency [Hz]')\n",
        "    #plt.ylabel('PSD')\n",
        "\n",
        "    # c has frequency and reading together\n",
        "    c =[]\n",
        "    for k in range(0,129):\n",
        "      x=[pnew[i][k],fnew[i][k]]\n",
        "      c.append(x)\n",
        "\n",
        "    #method1 = mean \n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] <=4.0):  #delta\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    deltamean.append(mean)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >4.0 and c[a][1] <=7.5 ): #theta\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    thetamean.append(mean)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >7.5 and c[a][1]<=13):   #alpha\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    alphamean.append(mean)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >13 and c[a][1]<=20):    #beta\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    betamean.append(mean)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >20 and c[a][1]<=40):    #gamma\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    gammamean.append(mean)\n",
        "  \n",
        "    fe=[]\n",
        "    fe.append(deltamean[i])\n",
        "    fe.append(thetamean[i])\n",
        "    fe.append(alphamean[i])\n",
        "    fe.append(betamean[i])\n",
        "    fe.append(gammamean[i])\n",
        "      \n",
        "    feature.append(fe)\n",
        "\n",
        "  return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyYxLLolx1uA"
      },
      "source": [
        "def feat_extract_EEG(raw, electrode):\n",
        "    EEG_tmp = np.zeros((23, 18, 60))\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "\n",
        "              B, S = [], []\n",
        "                \n",
        "              basl = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"baseline\"][0, 0][video, 0][:, electrode])\n",
        "              stim = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode])\n",
        "                \n",
        "              B = preprocess_EEG(basl, B) # baseline psd received\n",
        "              S = preprocess_EEG(stim, S) # stimulation psd received\n",
        "\n",
        "              A = np.divide(S, B)   # relative density(absolute is diff for each individual)\n",
        "              column = 0\n",
        "\n",
        "              for k in range(0,5):\n",
        "                for l in range(0,12):\n",
        "                  EEG_tmp[participant, video,column] = A[l][k]\n",
        "                  column+=1\n",
        "                                \n",
        "\n",
        "    col = []\n",
        "\n",
        "    for i in range(0, 12):\n",
        "      col.append(\"psddelta_\"+str(i + 1)+\"_un\")\n",
        "    for i in range(0, 12):\n",
        "      col.append(\"psdtheta_\"+str(i + 1)+\"_un\")\n",
        "    for i in range(0, 12):\n",
        "      col.append(\"psdalpha_\"+str(i + 1)+\"_un\")\n",
        "    for i in range(0, 12):\n",
        "     col.append(\"psdbeta_\"+str(i + 1)+\"_un\")\n",
        "    for i in range(0, 12):\n",
        "      col.append(\"psdgamma_\"+str(i + 1)+\"_un\")\n",
        "\n",
        "    print(EEG_tmp.shape)\n",
        "    EEG_tmp = EEG_tmp.reshape(-1,EEG_tmp.shape[2])\n",
        "    print(EEG_tmp.shape)\n",
        "    data_EEG = pd.DataFrame(EEG_tmp, columns=col)\n",
        "    #scaler = StandardScaler()\n",
        "    print(data_EEG)\n",
        "    print(col)\n",
        "    \n",
        "    for i in range(len(col)):\n",
        "        data_EEG[col[i][:-3]] = data_EEG[[col[i]]]\n",
        "        \n",
        "    data_EEG.drop(col, axis=1, inplace=True)\n",
        "    return data_EEG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrRPtLU_x1uC"
      },
      "source": [
        "def participant_affective(raw):\n",
        "    a = np.zeros((23, 18, 9), dtype=object)\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "            a[participant, video, 0] = (raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                        [0, participant][\"Age\"][0][0][0])\n",
        "            \n",
        "            a[participant, video, 1] = (raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                        [0, participant][\"Gender\"][0][0][0])\n",
        "            \n",
        "            a[participant, video, 2] = int(participant+1)\n",
        "            \n",
        "            a[participant, video, 3] = int(video+1)\n",
        "            \n",
        "            a[participant, video, 4] = [\"Searching for Bobby Fischer\",\n",
        "                                        \"D.O.A.\", \"The Hangover\", \"The Ring\",\n",
        "                                        \"300\", \"National Lampoon\\'s VanWilder\",\n",
        "                                        \"Wall-E\", \"Crash\", \"My Girl\",\n",
        "                                        \"The Fly\", \"Pride and Prejudice\",\n",
        "                                        \"Modern Times\", \"Remember the Titans\",\n",
        "                                        \"Gentlemans Agreement\", \"Psycho\",\n",
        "                                        \"The Bourne Identitiy\",\n",
        "                                        \"The Shawshank Redemption\",\n",
        "                                        \"The Departed\"][video]\n",
        "            \n",
        "            a[participant, video, 5] = [\"calmness\", \"surprise\", \"amusement\",\n",
        "                                        \"fear\", \"excitement\", \"disgust\",\n",
        "                                        \"happiness\", \"anger\", \"sadness\",\n",
        "                                        \"disgust\", \"calmness\", \"amusement\",\n",
        "                                        \"happiness\", \"anger\", \"fear\",\n",
        "                                        \"excitement\", \"sadness\",\n",
        "                                        \"surprise\"][video]\n",
        "            a[participant, video, 6] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreValence\"]\n",
        "                                           [0, 0][video, 0])\n",
        "            a[participant, video, 7] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreArousal\"]\n",
        "                                           [0, 0][video, 0])\n",
        "            a[participant, video, 8] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreDominance\"]\n",
        "                                           [0, 0][video, 0])\n",
        "    b = pd.DataFrame(a.reshape((23*18, a.shape[2])),\n",
        "                     columns=[\"age\", \"gender\", \"participant\",\n",
        "                              \"video\", \"video_name\", \"target_emotion\",\n",
        "                              \"valence\", \"arousal\", \"dominance\"])\n",
        "    return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb-QdQ4UIE3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "c8621aaa-4f61-40f0-c6ff-3eff732886e9"
      },
      "source": [
        "#RUN Each one by one till cell where is it savd as csv\n",
        "\n",
        "#def_EEG1 = feat_extract_EEG(raw, 0)\n",
        "#def_EEG2 = feat_extract_EEG(raw, 1)\n",
        "def_EEG3 = feat_extract_EEG(raw, 2)\n",
        "#def_EEG4 = feat_extract_EEG(raw, 3)\n",
        "#def_EEG5 = feat_extract_EEG(raw, 4)\n",
        "#def_EEG6 = feat_extract_EEG(raw, 5)\n",
        "#def_EEG7 = feat_extract_EEG(raw, 6)\n",
        "#def_EEG8 = feat_extract_EEG(raw, 7)\n",
        "#def_EEG9 = feat_extract_EEG(raw, 8)\n",
        "#def_EEG10 = feat_extract_EEG(raw, 9)\n",
        "#def_EEG11 = feat_extract_EEG(raw, 10)\n",
        "#def_EEG12 = feat_extract_EEG(raw, 11)\n",
        "#def_EEG13 = feat_extract_EEG(raw, 12)\n",
        "#def_EEG14 = feat_extract_EEG(raw, 13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23, 18, 60)\n",
            "(414, 60)\n",
            "     psddelta_1_un  psddelta_2_un  ...  psdgamma_11_un  psdgamma_12_un\n",
            "0         0.026661       0.128126  ...        0.882784        0.788258\n",
            "1         0.456918       0.854457  ...        0.982465        0.913652\n",
            "2         0.249260       0.278371  ...        0.862490        0.793047\n",
            "3         0.461071       0.972462  ...        0.893176        0.921639\n",
            "4         0.612918       0.397175  ...        1.012029        1.154223\n",
            "..             ...            ...  ...             ...             ...\n",
            "409       0.138540       0.740260  ...        1.291717        0.776517\n",
            "410       1.030576       0.510389  ...        0.771709        0.725069\n",
            "411       0.485331       1.065134  ...        0.930742        1.022116\n",
            "412       0.267938       1.659443  ...        0.743575        0.789763\n",
            "413       0.780949       1.080066  ...        0.474525        0.581657\n",
            "\n",
            "[414 rows x 60 columns]\n",
            "['psddelta_1_un', 'psddelta_2_un', 'psddelta_3_un', 'psddelta_4_un', 'psddelta_5_un', 'psddelta_6_un', 'psddelta_7_un', 'psddelta_8_un', 'psddelta_9_un', 'psddelta_10_un', 'psddelta_11_un', 'psddelta_12_un', 'psdtheta_1_un', 'psdtheta_2_un', 'psdtheta_3_un', 'psdtheta_4_un', 'psdtheta_5_un', 'psdtheta_6_un', 'psdtheta_7_un', 'psdtheta_8_un', 'psdtheta_9_un', 'psdtheta_10_un', 'psdtheta_11_un', 'psdtheta_12_un', 'psdalpha_1_un', 'psdalpha_2_un', 'psdalpha_3_un', 'psdalpha_4_un', 'psdalpha_5_un', 'psdalpha_6_un', 'psdalpha_7_un', 'psdalpha_8_un', 'psdalpha_9_un', 'psdalpha_10_un', 'psdalpha_11_un', 'psdalpha_12_un', 'psdbeta_1_un', 'psdbeta_2_un', 'psdbeta_3_un', 'psdbeta_4_un', 'psdbeta_5_un', 'psdbeta_6_un', 'psdbeta_7_un', 'psdbeta_8_un', 'psdbeta_9_un', 'psdbeta_10_un', 'psdbeta_11_un', 'psdbeta_12_un', 'psdgamma_1_un', 'psdgamma_2_un', 'psdgamma_3_un', 'psdgamma_4_un', 'psdgamma_5_un', 'psdgamma_6_un', 'psdgamma_7_un', 'psdgamma_8_un', 'psdgamma_9_un', 'psdgamma_10_un', 'psdgamma_11_un', 'psdgamma_12_un']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHjGGyUDG-Hb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "ac7f62a1-77d7-47e7-f045-199d92c37318"
      },
      "source": [
        "#Enter desired electrode\n",
        "df_features = def_EEG3\n",
        "\n",
        "df_participant_affective = participant_affective(raw)\n",
        "\n",
        "# not really clear to me why I have to do this because I thought these were already integers\n",
        "df_participant_affective[\"valence\"] = (df_participant_affective\n",
        "                                       [\"valence\"].astype(int))\n",
        "df_participant_affective[\"arousal\"] = (df_participant_affective\n",
        "                                       [\"arousal\"].astype(int))\n",
        "df_participant_affective[\"dominance\"] = (df_participant_affective\n",
        "                                         [\"dominance\"].astype(int))\n",
        "\n",
        "df = pd.concat([df_features, df_participant_affective], axis=1)\n",
        "df2 = pd.concat([df_features, df_participant_affective], axis=1) ##\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>psddelta_1</th>\n",
              "      <th>psddelta_2</th>\n",
              "      <th>psddelta_3</th>\n",
              "      <th>psddelta_4</th>\n",
              "      <th>psddelta_5</th>\n",
              "      <th>psddelta_6</th>\n",
              "      <th>psddelta_7</th>\n",
              "      <th>psddelta_8</th>\n",
              "      <th>psddelta_9</th>\n",
              "      <th>psddelta_10</th>\n",
              "      <th>psddelta_11</th>\n",
              "      <th>psddelta_12</th>\n",
              "      <th>psdtheta_1</th>\n",
              "      <th>psdtheta_2</th>\n",
              "      <th>psdtheta_3</th>\n",
              "      <th>psdtheta_4</th>\n",
              "      <th>psdtheta_5</th>\n",
              "      <th>psdtheta_6</th>\n",
              "      <th>psdtheta_7</th>\n",
              "      <th>psdtheta_8</th>\n",
              "      <th>psdtheta_9</th>\n",
              "      <th>psdtheta_10</th>\n",
              "      <th>psdtheta_11</th>\n",
              "      <th>psdtheta_12</th>\n",
              "      <th>psdalpha_1</th>\n",
              "      <th>psdalpha_2</th>\n",
              "      <th>psdalpha_3</th>\n",
              "      <th>psdalpha_4</th>\n",
              "      <th>psdalpha_5</th>\n",
              "      <th>psdalpha_6</th>\n",
              "      <th>psdalpha_7</th>\n",
              "      <th>psdalpha_8</th>\n",
              "      <th>psdalpha_9</th>\n",
              "      <th>psdalpha_10</th>\n",
              "      <th>psdalpha_11</th>\n",
              "      <th>psdalpha_12</th>\n",
              "      <th>psdbeta_1</th>\n",
              "      <th>psdbeta_2</th>\n",
              "      <th>psdbeta_3</th>\n",
              "      <th>psdbeta_4</th>\n",
              "      <th>psdbeta_5</th>\n",
              "      <th>psdbeta_6</th>\n",
              "      <th>psdbeta_7</th>\n",
              "      <th>psdbeta_8</th>\n",
              "      <th>psdbeta_9</th>\n",
              "      <th>psdbeta_10</th>\n",
              "      <th>psdbeta_11</th>\n",
              "      <th>psdbeta_12</th>\n",
              "      <th>psdgamma_1</th>\n",
              "      <th>psdgamma_2</th>\n",
              "      <th>psdgamma_3</th>\n",
              "      <th>psdgamma_4</th>\n",
              "      <th>psdgamma_5</th>\n",
              "      <th>psdgamma_6</th>\n",
              "      <th>psdgamma_7</th>\n",
              "      <th>psdgamma_8</th>\n",
              "      <th>psdgamma_9</th>\n",
              "      <th>psdgamma_10</th>\n",
              "      <th>psdgamma_11</th>\n",
              "      <th>psdgamma_12</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>participant</th>\n",
              "      <th>video</th>\n",
              "      <th>video_name</th>\n",
              "      <th>target_emotion</th>\n",
              "      <th>valence</th>\n",
              "      <th>arousal</th>\n",
              "      <th>dominance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026661</td>\n",
              "      <td>0.128126</td>\n",
              "      <td>0.263066</td>\n",
              "      <td>0.205762</td>\n",
              "      <td>2.020957</td>\n",
              "      <td>0.382125</td>\n",
              "      <td>0.386174</td>\n",
              "      <td>0.393051</td>\n",
              "      <td>0.489584</td>\n",
              "      <td>0.697933</td>\n",
              "      <td>0.134432</td>\n",
              "      <td>0.248278</td>\n",
              "      <td>0.245683</td>\n",
              "      <td>0.540516</td>\n",
              "      <td>0.550646</td>\n",
              "      <td>0.554948</td>\n",
              "      <td>0.515048</td>\n",
              "      <td>0.767026</td>\n",
              "      <td>0.485870</td>\n",
              "      <td>0.663862</td>\n",
              "      <td>0.900809</td>\n",
              "      <td>0.714160</td>\n",
              "      <td>0.348549</td>\n",
              "      <td>0.528158</td>\n",
              "      <td>0.558594</td>\n",
              "      <td>0.750520</td>\n",
              "      <td>0.349834</td>\n",
              "      <td>0.942867</td>\n",
              "      <td>0.611311</td>\n",
              "      <td>0.879640</td>\n",
              "      <td>0.775826</td>\n",
              "      <td>0.881580</td>\n",
              "      <td>0.847456</td>\n",
              "      <td>1.301150</td>\n",
              "      <td>0.846869</td>\n",
              "      <td>0.613916</td>\n",
              "      <td>0.336742</td>\n",
              "      <td>0.680048</td>\n",
              "      <td>0.866783</td>\n",
              "      <td>1.402146</td>\n",
              "      <td>1.049143</td>\n",
              "      <td>0.796938</td>\n",
              "      <td>0.741151</td>\n",
              "      <td>1.296224</td>\n",
              "      <td>1.018292</td>\n",
              "      <td>1.565926</td>\n",
              "      <td>0.735186</td>\n",
              "      <td>0.765550</td>\n",
              "      <td>0.578959</td>\n",
              "      <td>0.819340</td>\n",
              "      <td>0.739843</td>\n",
              "      <td>0.820724</td>\n",
              "      <td>0.682887</td>\n",
              "      <td>0.872524</td>\n",
              "      <td>0.848397</td>\n",
              "      <td>0.767168</td>\n",
              "      <td>0.750893</td>\n",
              "      <td>0.875274</td>\n",
              "      <td>0.882784</td>\n",
              "      <td>0.788258</td>\n",
              "      <td>22</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Searching for Bobby Fischer</td>\n",
              "      <td>calmness</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.456918</td>\n",
              "      <td>0.854457</td>\n",
              "      <td>1.389768</td>\n",
              "      <td>0.418616</td>\n",
              "      <td>0.444282</td>\n",
              "      <td>1.150945</td>\n",
              "      <td>1.379311</td>\n",
              "      <td>0.951290</td>\n",
              "      <td>0.990952</td>\n",
              "      <td>0.587898</td>\n",
              "      <td>0.526920</td>\n",
              "      <td>0.597184</td>\n",
              "      <td>0.541314</td>\n",
              "      <td>0.613755</td>\n",
              "      <td>0.607566</td>\n",
              "      <td>0.956216</td>\n",
              "      <td>0.255011</td>\n",
              "      <td>0.274969</td>\n",
              "      <td>0.257066</td>\n",
              "      <td>0.411898</td>\n",
              "      <td>0.334922</td>\n",
              "      <td>0.414285</td>\n",
              "      <td>1.015785</td>\n",
              "      <td>0.683988</td>\n",
              "      <td>0.626123</td>\n",
              "      <td>0.660259</td>\n",
              "      <td>1.511112</td>\n",
              "      <td>0.823077</td>\n",
              "      <td>0.456709</td>\n",
              "      <td>0.415401</td>\n",
              "      <td>0.860516</td>\n",
              "      <td>0.402102</td>\n",
              "      <td>0.950805</td>\n",
              "      <td>1.119504</td>\n",
              "      <td>0.381495</td>\n",
              "      <td>0.398225</td>\n",
              "      <td>0.832206</td>\n",
              "      <td>0.519201</td>\n",
              "      <td>0.531851</td>\n",
              "      <td>0.800187</td>\n",
              "      <td>0.745460</td>\n",
              "      <td>0.676006</td>\n",
              "      <td>0.714218</td>\n",
              "      <td>0.768573</td>\n",
              "      <td>0.751424</td>\n",
              "      <td>0.531997</td>\n",
              "      <td>1.120774</td>\n",
              "      <td>0.819686</td>\n",
              "      <td>1.026307</td>\n",
              "      <td>0.810284</td>\n",
              "      <td>0.974069</td>\n",
              "      <td>0.798344</td>\n",
              "      <td>0.839761</td>\n",
              "      <td>0.887248</td>\n",
              "      <td>1.083175</td>\n",
              "      <td>0.968977</td>\n",
              "      <td>0.874396</td>\n",
              "      <td>0.855578</td>\n",
              "      <td>0.982465</td>\n",
              "      <td>0.913652</td>\n",
              "      <td>22</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>D.O.A.</td>\n",
              "      <td>surprise</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.249260</td>\n",
              "      <td>0.278371</td>\n",
              "      <td>0.645247</td>\n",
              "      <td>0.561594</td>\n",
              "      <td>0.405477</td>\n",
              "      <td>1.393356</td>\n",
              "      <td>0.415535</td>\n",
              "      <td>61.628447</td>\n",
              "      <td>54.163474</td>\n",
              "      <td>0.894641</td>\n",
              "      <td>0.655756</td>\n",
              "      <td>0.577133</td>\n",
              "      <td>0.021824</td>\n",
              "      <td>0.933490</td>\n",
              "      <td>0.299910</td>\n",
              "      <td>0.710720</td>\n",
              "      <td>0.936751</td>\n",
              "      <td>0.672708</td>\n",
              "      <td>0.825130</td>\n",
              "      <td>50.162477</td>\n",
              "      <td>54.494428</td>\n",
              "      <td>0.761028</td>\n",
              "      <td>0.958375</td>\n",
              "      <td>1.421353</td>\n",
              "      <td>0.098920</td>\n",
              "      <td>0.559696</td>\n",
              "      <td>0.529725</td>\n",
              "      <td>0.697565</td>\n",
              "      <td>0.550774</td>\n",
              "      <td>0.885256</td>\n",
              "      <td>0.586864</td>\n",
              "      <td>49.798175</td>\n",
              "      <td>97.586580</td>\n",
              "      <td>0.722648</td>\n",
              "      <td>0.908019</td>\n",
              "      <td>0.654330</td>\n",
              "      <td>0.407342</td>\n",
              "      <td>0.540492</td>\n",
              "      <td>0.980841</td>\n",
              "      <td>0.504885</td>\n",
              "      <td>0.598663</td>\n",
              "      <td>0.427006</td>\n",
              "      <td>1.319814</td>\n",
              "      <td>8.104653</td>\n",
              "      <td>111.160500</td>\n",
              "      <td>0.708537</td>\n",
              "      <td>0.672024</td>\n",
              "      <td>1.136864</td>\n",
              "      <td>0.782554</td>\n",
              "      <td>1.020950</td>\n",
              "      <td>0.966922</td>\n",
              "      <td>0.727917</td>\n",
              "      <td>0.945849</td>\n",
              "      <td>0.722588</td>\n",
              "      <td>0.686958</td>\n",
              "      <td>1.280376</td>\n",
              "      <td>2.198474</td>\n",
              "      <td>0.802747</td>\n",
              "      <td>0.862490</td>\n",
              "      <td>0.793047</td>\n",
              "      <td>22</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>The Hangover</td>\n",
              "      <td>amusement</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.461071</td>\n",
              "      <td>0.972462</td>\n",
              "      <td>0.878281</td>\n",
              "      <td>0.657676</td>\n",
              "      <td>1.148692</td>\n",
              "      <td>0.881123</td>\n",
              "      <td>0.523188</td>\n",
              "      <td>0.636787</td>\n",
              "      <td>1.049582</td>\n",
              "      <td>0.879103</td>\n",
              "      <td>1.341587</td>\n",
              "      <td>1.477064</td>\n",
              "      <td>0.181619</td>\n",
              "      <td>0.798121</td>\n",
              "      <td>0.534931</td>\n",
              "      <td>0.386719</td>\n",
              "      <td>1.022062</td>\n",
              "      <td>0.870922</td>\n",
              "      <td>0.667982</td>\n",
              "      <td>0.716668</td>\n",
              "      <td>0.401747</td>\n",
              "      <td>0.679756</td>\n",
              "      <td>0.726421</td>\n",
              "      <td>0.755361</td>\n",
              "      <td>0.574776</td>\n",
              "      <td>0.493555</td>\n",
              "      <td>0.899483</td>\n",
              "      <td>0.821588</td>\n",
              "      <td>0.951741</td>\n",
              "      <td>0.861824</td>\n",
              "      <td>0.792229</td>\n",
              "      <td>1.448838</td>\n",
              "      <td>1.073606</td>\n",
              "      <td>1.218513</td>\n",
              "      <td>1.188021</td>\n",
              "      <td>1.169386</td>\n",
              "      <td>0.598369</td>\n",
              "      <td>0.757336</td>\n",
              "      <td>1.161237</td>\n",
              "      <td>0.479530</td>\n",
              "      <td>0.563567</td>\n",
              "      <td>0.751573</td>\n",
              "      <td>0.908230</td>\n",
              "      <td>0.649187</td>\n",
              "      <td>0.758692</td>\n",
              "      <td>0.929840</td>\n",
              "      <td>0.641589</td>\n",
              "      <td>0.900956</td>\n",
              "      <td>0.996206</td>\n",
              "      <td>0.761949</td>\n",
              "      <td>0.995205</td>\n",
              "      <td>0.790317</td>\n",
              "      <td>0.754718</td>\n",
              "      <td>0.876863</td>\n",
              "      <td>0.836408</td>\n",
              "      <td>1.001039</td>\n",
              "      <td>0.892834</td>\n",
              "      <td>0.883298</td>\n",
              "      <td>0.893176</td>\n",
              "      <td>0.921639</td>\n",
              "      <td>22</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>The Ring</td>\n",
              "      <td>fear</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.612918</td>\n",
              "      <td>0.397175</td>\n",
              "      <td>0.419856</td>\n",
              "      <td>0.409736</td>\n",
              "      <td>0.800813</td>\n",
              "      <td>157.594122</td>\n",
              "      <td>7.629884</td>\n",
              "      <td>2.357575</td>\n",
              "      <td>1.201276</td>\n",
              "      <td>0.801799</td>\n",
              "      <td>0.990044</td>\n",
              "      <td>0.097768</td>\n",
              "      <td>0.904371</td>\n",
              "      <td>0.833003</td>\n",
              "      <td>0.558178</td>\n",
              "      <td>0.529204</td>\n",
              "      <td>0.493676</td>\n",
              "      <td>3.089034</td>\n",
              "      <td>0.773680</td>\n",
              "      <td>0.948897</td>\n",
              "      <td>1.356528</td>\n",
              "      <td>1.986031</td>\n",
              "      <td>1.060070</td>\n",
              "      <td>0.074344</td>\n",
              "      <td>0.902573</td>\n",
              "      <td>0.462086</td>\n",
              "      <td>0.912282</td>\n",
              "      <td>0.787960</td>\n",
              "      <td>1.126032</td>\n",
              "      <td>1.995489</td>\n",
              "      <td>0.870255</td>\n",
              "      <td>0.616019</td>\n",
              "      <td>0.568001</td>\n",
              "      <td>0.573661</td>\n",
              "      <td>0.539903</td>\n",
              "      <td>0.103279</td>\n",
              "      <td>0.845877</td>\n",
              "      <td>0.531877</td>\n",
              "      <td>0.978822</td>\n",
              "      <td>0.810316</td>\n",
              "      <td>0.657459</td>\n",
              "      <td>0.805898</td>\n",
              "      <td>0.643490</td>\n",
              "      <td>0.952502</td>\n",
              "      <td>0.998672</td>\n",
              "      <td>0.792066</td>\n",
              "      <td>0.471923</td>\n",
              "      <td>0.306508</td>\n",
              "      <td>0.934004</td>\n",
              "      <td>0.979796</td>\n",
              "      <td>1.083026</td>\n",
              "      <td>0.945325</td>\n",
              "      <td>1.125120</td>\n",
              "      <td>1.462209</td>\n",
              "      <td>1.538712</td>\n",
              "      <td>0.982351</td>\n",
              "      <td>1.034477</td>\n",
              "      <td>0.944886</td>\n",
              "      <td>1.012029</td>\n",
              "      <td>1.154223</td>\n",
              "      <td>22</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>300</td>\n",
              "      <td>excitement</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>0.138540</td>\n",
              "      <td>0.740260</td>\n",
              "      <td>0.137076</td>\n",
              "      <td>0.722844</td>\n",
              "      <td>0.729615</td>\n",
              "      <td>2.028608</td>\n",
              "      <td>1.186617</td>\n",
              "      <td>1.827346</td>\n",
              "      <td>1.400996</td>\n",
              "      <td>0.873736</td>\n",
              "      <td>1.808074</td>\n",
              "      <td>0.626639</td>\n",
              "      <td>0.758948</td>\n",
              "      <td>0.634545</td>\n",
              "      <td>0.698587</td>\n",
              "      <td>0.644718</td>\n",
              "      <td>0.684177</td>\n",
              "      <td>1.107732</td>\n",
              "      <td>0.828845</td>\n",
              "      <td>0.681730</td>\n",
              "      <td>1.879097</td>\n",
              "      <td>0.478986</td>\n",
              "      <td>1.314125</td>\n",
              "      <td>0.791682</td>\n",
              "      <td>1.235736</td>\n",
              "      <td>0.809184</td>\n",
              "      <td>1.092873</td>\n",
              "      <td>0.350075</td>\n",
              "      <td>0.545960</td>\n",
              "      <td>0.528902</td>\n",
              "      <td>0.425083</td>\n",
              "      <td>0.704372</td>\n",
              "      <td>0.377006</td>\n",
              "      <td>0.655934</td>\n",
              "      <td>0.446340</td>\n",
              "      <td>0.472249</td>\n",
              "      <td>0.520935</td>\n",
              "      <td>0.556933</td>\n",
              "      <td>0.779724</td>\n",
              "      <td>0.914343</td>\n",
              "      <td>0.811030</td>\n",
              "      <td>0.870350</td>\n",
              "      <td>1.394808</td>\n",
              "      <td>1.457416</td>\n",
              "      <td>1.887233</td>\n",
              "      <td>1.055075</td>\n",
              "      <td>1.266834</td>\n",
              "      <td>0.699005</td>\n",
              "      <td>0.293956</td>\n",
              "      <td>0.804165</td>\n",
              "      <td>0.872369</td>\n",
              "      <td>1.015052</td>\n",
              "      <td>0.773466</td>\n",
              "      <td>1.355194</td>\n",
              "      <td>0.935928</td>\n",
              "      <td>1.132655</td>\n",
              "      <td>1.370539</td>\n",
              "      <td>0.946168</td>\n",
              "      <td>1.291717</td>\n",
              "      <td>0.776517</td>\n",
              "      <td>25</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>14</td>\n",
              "      <td>Gentlemans Agreement</td>\n",
              "      <td>anger</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>1.030576</td>\n",
              "      <td>0.510389</td>\n",
              "      <td>1.223502</td>\n",
              "      <td>0.969153</td>\n",
              "      <td>0.888137</td>\n",
              "      <td>2.131785</td>\n",
              "      <td>0.732007</td>\n",
              "      <td>0.315462</td>\n",
              "      <td>0.985302</td>\n",
              "      <td>4.660665</td>\n",
              "      <td>0.728370</td>\n",
              "      <td>2.172472</td>\n",
              "      <td>0.581908</td>\n",
              "      <td>1.397014</td>\n",
              "      <td>1.163427</td>\n",
              "      <td>1.571878</td>\n",
              "      <td>2.081589</td>\n",
              "      <td>1.642985</td>\n",
              "      <td>2.708716</td>\n",
              "      <td>1.718136</td>\n",
              "      <td>1.496718</td>\n",
              "      <td>0.995791</td>\n",
              "      <td>1.160647</td>\n",
              "      <td>1.089294</td>\n",
              "      <td>0.378880</td>\n",
              "      <td>1.095830</td>\n",
              "      <td>1.287236</td>\n",
              "      <td>1.279129</td>\n",
              "      <td>1.081806</td>\n",
              "      <td>2.039075</td>\n",
              "      <td>2.083952</td>\n",
              "      <td>1.244721</td>\n",
              "      <td>0.466243</td>\n",
              "      <td>0.671358</td>\n",
              "      <td>1.067043</td>\n",
              "      <td>1.157593</td>\n",
              "      <td>0.574956</td>\n",
              "      <td>0.622328</td>\n",
              "      <td>0.739490</td>\n",
              "      <td>1.962890</td>\n",
              "      <td>0.466973</td>\n",
              "      <td>0.978233</td>\n",
              "      <td>0.348019</td>\n",
              "      <td>0.629352</td>\n",
              "      <td>0.684920</td>\n",
              "      <td>0.572640</td>\n",
              "      <td>1.280119</td>\n",
              "      <td>0.801108</td>\n",
              "      <td>0.552267</td>\n",
              "      <td>0.652137</td>\n",
              "      <td>0.559147</td>\n",
              "      <td>0.676802</td>\n",
              "      <td>0.561795</td>\n",
              "      <td>0.457698</td>\n",
              "      <td>0.599756</td>\n",
              "      <td>0.480172</td>\n",
              "      <td>0.682845</td>\n",
              "      <td>0.808045</td>\n",
              "      <td>0.771709</td>\n",
              "      <td>0.725069</td>\n",
              "      <td>25</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>Psycho</td>\n",
              "      <td>fear</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>0.485331</td>\n",
              "      <td>1.065134</td>\n",
              "      <td>0.823158</td>\n",
              "      <td>2.184300</td>\n",
              "      <td>0.973482</td>\n",
              "      <td>0.991726</td>\n",
              "      <td>1.113113</td>\n",
              "      <td>0.746018</td>\n",
              "      <td>0.610463</td>\n",
              "      <td>0.372020</td>\n",
              "      <td>0.404188</td>\n",
              "      <td>1.744929</td>\n",
              "      <td>1.194251</td>\n",
              "      <td>1.100467</td>\n",
              "      <td>0.955959</td>\n",
              "      <td>1.179948</td>\n",
              "      <td>1.128991</td>\n",
              "      <td>2.386312</td>\n",
              "      <td>1.933240</td>\n",
              "      <td>1.489888</td>\n",
              "      <td>1.403353</td>\n",
              "      <td>0.737067</td>\n",
              "      <td>0.917269</td>\n",
              "      <td>1.362166</td>\n",
              "      <td>1.771519</td>\n",
              "      <td>0.807734</td>\n",
              "      <td>0.859592</td>\n",
              "      <td>0.442489</td>\n",
              "      <td>0.897526</td>\n",
              "      <td>0.803444</td>\n",
              "      <td>0.595802</td>\n",
              "      <td>0.416633</td>\n",
              "      <td>0.519730</td>\n",
              "      <td>0.508404</td>\n",
              "      <td>1.122510</td>\n",
              "      <td>1.726274</td>\n",
              "      <td>0.985074</td>\n",
              "      <td>1.013943</td>\n",
              "      <td>0.636979</td>\n",
              "      <td>1.414672</td>\n",
              "      <td>0.969926</td>\n",
              "      <td>0.500058</td>\n",
              "      <td>1.162636</td>\n",
              "      <td>0.820783</td>\n",
              "      <td>1.154433</td>\n",
              "      <td>0.752011</td>\n",
              "      <td>0.746734</td>\n",
              "      <td>0.995994</td>\n",
              "      <td>1.252884</td>\n",
              "      <td>1.582162</td>\n",
              "      <td>1.215747</td>\n",
              "      <td>1.553863</td>\n",
              "      <td>0.981326</td>\n",
              "      <td>1.178118</td>\n",
              "      <td>0.927384</td>\n",
              "      <td>1.003817</td>\n",
              "      <td>0.764340</td>\n",
              "      <td>1.056210</td>\n",
              "      <td>0.930742</td>\n",
              "      <td>1.022116</td>\n",
              "      <td>25</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>16</td>\n",
              "      <td>The Bourne Identitiy</td>\n",
              "      <td>excitement</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>0.267938</td>\n",
              "      <td>1.659443</td>\n",
              "      <td>1.438146</td>\n",
              "      <td>2.774620</td>\n",
              "      <td>1.357219</td>\n",
              "      <td>0.906445</td>\n",
              "      <td>2.320216</td>\n",
              "      <td>1.774690</td>\n",
              "      <td>2.377467</td>\n",
              "      <td>0.718333</td>\n",
              "      <td>1.213684</td>\n",
              "      <td>0.526761</td>\n",
              "      <td>0.660676</td>\n",
              "      <td>5.041791</td>\n",
              "      <td>2.598664</td>\n",
              "      <td>1.228213</td>\n",
              "      <td>0.482363</td>\n",
              "      <td>0.670656</td>\n",
              "      <td>0.675171</td>\n",
              "      <td>2.275624</td>\n",
              "      <td>1.649799</td>\n",
              "      <td>1.544247</td>\n",
              "      <td>1.389609</td>\n",
              "      <td>0.601207</td>\n",
              "      <td>1.446953</td>\n",
              "      <td>0.870871</td>\n",
              "      <td>0.450526</td>\n",
              "      <td>0.372226</td>\n",
              "      <td>0.581788</td>\n",
              "      <td>0.408065</td>\n",
              "      <td>0.380096</td>\n",
              "      <td>1.200043</td>\n",
              "      <td>0.503610</td>\n",
              "      <td>1.808501</td>\n",
              "      <td>0.863161</td>\n",
              "      <td>0.725556</td>\n",
              "      <td>0.764374</td>\n",
              "      <td>0.361915</td>\n",
              "      <td>0.867546</td>\n",
              "      <td>0.721928</td>\n",
              "      <td>0.477149</td>\n",
              "      <td>1.134181</td>\n",
              "      <td>0.475078</td>\n",
              "      <td>2.395561</td>\n",
              "      <td>1.427553</td>\n",
              "      <td>1.092744</td>\n",
              "      <td>0.780654</td>\n",
              "      <td>0.598990</td>\n",
              "      <td>0.400015</td>\n",
              "      <td>0.192885</td>\n",
              "      <td>0.657294</td>\n",
              "      <td>0.708347</td>\n",
              "      <td>1.045789</td>\n",
              "      <td>1.062519</td>\n",
              "      <td>0.832245</td>\n",
              "      <td>0.881281</td>\n",
              "      <td>0.665699</td>\n",
              "      <td>1.305362</td>\n",
              "      <td>0.743575</td>\n",
              "      <td>0.789763</td>\n",
              "      <td>25</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>17</td>\n",
              "      <td>The Shawshank Redemption</td>\n",
              "      <td>sadness</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>0.780949</td>\n",
              "      <td>1.080066</td>\n",
              "      <td>2.682278</td>\n",
              "      <td>3.388246</td>\n",
              "      <td>1.539429</td>\n",
              "      <td>1.797761</td>\n",
              "      <td>1.465806</td>\n",
              "      <td>1.211867</td>\n",
              "      <td>1.342021</td>\n",
              "      <td>2.177064</td>\n",
              "      <td>1.415317</td>\n",
              "      <td>1.658770</td>\n",
              "      <td>1.976663</td>\n",
              "      <td>0.716955</td>\n",
              "      <td>0.674721</td>\n",
              "      <td>1.006394</td>\n",
              "      <td>0.895461</td>\n",
              "      <td>1.203036</td>\n",
              "      <td>1.410720</td>\n",
              "      <td>1.326621</td>\n",
              "      <td>1.336854</td>\n",
              "      <td>1.240247</td>\n",
              "      <td>1.130537</td>\n",
              "      <td>1.313661</td>\n",
              "      <td>1.141490</td>\n",
              "      <td>2.113747</td>\n",
              "      <td>0.650762</td>\n",
              "      <td>0.631919</td>\n",
              "      <td>0.164760</td>\n",
              "      <td>0.565192</td>\n",
              "      <td>0.794255</td>\n",
              "      <td>1.300830</td>\n",
              "      <td>0.575623</td>\n",
              "      <td>1.838810</td>\n",
              "      <td>1.071255</td>\n",
              "      <td>0.380353</td>\n",
              "      <td>0.729466</td>\n",
              "      <td>0.746002</td>\n",
              "      <td>0.794803</td>\n",
              "      <td>0.760960</td>\n",
              "      <td>0.658976</td>\n",
              "      <td>1.126780</td>\n",
              "      <td>0.718750</td>\n",
              "      <td>1.137485</td>\n",
              "      <td>0.559066</td>\n",
              "      <td>0.886901</td>\n",
              "      <td>0.938086</td>\n",
              "      <td>0.497507</td>\n",
              "      <td>0.442304</td>\n",
              "      <td>0.769655</td>\n",
              "      <td>0.566586</td>\n",
              "      <td>0.437114</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.581302</td>\n",
              "      <td>0.541759</td>\n",
              "      <td>0.548634</td>\n",
              "      <td>0.527461</td>\n",
              "      <td>0.597240</td>\n",
              "      <td>0.474525</td>\n",
              "      <td>0.581657</td>\n",
              "      <td>25</td>\n",
              "      <td>male</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>The Departed</td>\n",
              "      <td>surprise</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>414 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     psddelta_1  psddelta_2  psddelta_3  ...  valence  arousal  dominance\n",
              "0      0.026661    0.128126    0.263066  ...        4        3          2\n",
              "1      0.456918    0.854457    1.389768  ...        3        3          1\n",
              "2      0.249260    0.278371    0.645247  ...        5        4          4\n",
              "3      0.461071    0.972462    0.878281  ...        4        3          2\n",
              "4      0.612918    0.397175    0.419856  ...        4        4          4\n",
              "..          ...         ...         ...  ...      ...      ...        ...\n",
              "409    0.138540    0.740260    0.137076  ...        2        2          2\n",
              "410    1.030576    0.510389    1.223502  ...        2        2          2\n",
              "411    0.485331    1.065134    0.823158  ...        3        3          2\n",
              "412    0.267938    1.659443    1.438146  ...        2        2          4\n",
              "413    0.780949    1.080066    2.682278  ...        2        4          2\n",
              "\n",
              "[414 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSyb0Lej1IAR"
      },
      "source": [
        "data2 = df2.loc[(df['target_emotion'] == 'anger') |\n",
        "                (df['target_emotion'] == 'fear') |\n",
        "                (df['target_emotion'] == 'calmness') |\n",
        "                (df['target_emotion'] == 'surprise') |\n",
        "                (df['target_emotion'] == 'excitement') |\n",
        "                (df['target_emotion'] == 'amusement') |\n",
        "                (df['target_emotion'] == 'happiness') |\n",
        "                (df['target_emotion'] == 'sadness') |\n",
        "                (df['target_emotion'] == 'disgust')].copy()\n",
        "\n",
        "d={'surprise': 0, 'excitement': 0, 'amusement': 0, 'happiness': 0, 'fear': 1, 'anger': 1, 'sadness':2, 'disgust':2 ,'calmness': 3}\n",
        "data2['class'] = data2.target_emotion.map(d)\n",
        "\n",
        "e={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "data2['valencehigh'] = data2.valence.map(e)\n",
        "\n",
        "f={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "data2['arousalhigh'] = data2.arousal.map(e)\n",
        "\n",
        "# add another column with title 'stress_bin' and give it value 0/1 depending on what emotion it elicits\n",
        "#data['stress_bin'] = data['target_emotion'].map({'anger': 1, 'fear': 1, 'calmness': 0})\n",
        "\n",
        "# save features, demographic, and emotion data as csv\n",
        "data2.to_csv('DE3_NS.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAvsSOoz-FH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "742d874d-9d7d-4afe-bcec-f0651d6d8e6d"
      },
      "source": [
        "data2 = pd.read_csv(\"ml_data_untouched.csv\")  \n",
        "data_2 = data2.head()\n",
        "data_2  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4f267a34c6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml_data_untouched.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY4CY9M5HmvA"
      },
      "source": [
        "## SOME CHECKS\n",
        "# print( type(data))\n",
        "#data = pd.read_csv(\"ml_data.csv\")  \n",
        "#data_ = data.head()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYiN_a6EfT9E"
      },
      "source": [
        "#DATA PREP FOR VALENCE\n",
        "\n",
        "#data  - ml_data.csv\n",
        "#data2 - ml_data_untouched.csv\n",
        "#Valence is judged by alphas of F3 and F4\n",
        "#F3 is data 3, F4 is data 12\n",
        "\n",
        "#df1=data2.reset_index()['psdalpha_12']\n",
        "#df2=data2.reset_index()['psdalpha_3']\n",
        "#df1 = np.array(df1, dtype = float)\n",
        "#df2 = np.array(df2, dtype = float)\n",
        "#df3 = np.concatenate((df1, df2), axis=0)\n",
        "#data2=data2.reset_index()\n",
        "dfz=data2.iloc[:,1:43]\n",
        "#dfy=pd.DataFrame(data={ 'alpha12':df1,'alpha3':df2})\n",
        "#df1 = df1.append(df2)\n",
        "dfz = np.array(dfz, dtype = float)\n",
        "dfz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bRYlgXuP45"
      },
      "source": [
        "target=data2['valencehigh']\n",
        "#target = target.append(target)\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5fSvH3gvKMD"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#df1 = np.array(df1, dtype = float)\n",
        "#df2 = np.array(df2, dtype = float)\n",
        "target = np.array(target, dtype = float)\n",
        "\n",
        "\n",
        "x_Train, x_Test, y_Train, y_Test = train_test_split(dfz, target, test_size = 0.35, random_state = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9os0YpZ8IhwA"
      },
      "source": [
        "x_Train = x_Train.reshape(x_Train.shape[0],1,42)\n",
        "x_Test = x_Test.reshape(x_Test.shape[0],1,42)\n",
        "x_Test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxNjvs3QITTf"
      },
      "source": [
        "import tensorflow.keras.layers as KL\n",
        "inputs = KL.Input(shape=(1,42))\n",
        "x = KL.LSTM(units = 8, activation = 'relu')(inputs)\n",
        "outputs = KL.Dense(units=1, activation= 'sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs, outputs)\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "model.compile( optimizer=opt , loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "history = model.fit(x_Train, y_Train, batch_size=20, epochs=1000, class_weight={0:1, 1:2},validation_data =(x_Test,y_Test))\n",
        "\n",
        "#y_pred = model.predict(x_Test)\n",
        "#print(y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RiAIH_ELCnd"
      },
      "source": [
        "#from sklearn.metrics import confusion_matrix, classification_report,mean_squared_error\n",
        "#confudf = pd.DataFrame(confusion_matrix(y_Test,y_pred), index = [\"Negative (Actual)\",\"Positive (Actual)\"], columns=['Negative (Predicted)', 'Postive (Predicted)'])\n",
        "#text = np.asarray([['True Negative', 'False Positive'], ['False Negative', 'True Positive']])\n",
        "#labels = (np.asarray([\"{0}\\n{1:.2f}\".format(text,data) for text, data in zip(text.flatten(), confusion_matrix(y_Test,y_pred).flatten())])).reshape(2,2)\n",
        "\n",
        "\n",
        "#sns.heatmap(confudf, annot = labels, center=100, fmt='', linewidth = '1', vmax=300, cmap = \"Blues\", cbar=False)\n",
        "\n",
        "#plt.yticks(rotation = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clJ3o2udvkVg"
      },
      "source": [
        "#model for Valence -LSTM\n",
        "\n",
        "#x_Train = x_Train.reshape(x_Train.shape[0],1,1)\n",
        "#x_Test = x_Test.reshape(x_Test.shape[0],1,1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128,activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,activation ='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32,activation ='relu'))\n",
        "model.add(Dense(2,activation ='sigmoid'))\n",
        "y_pred = model.predict(x_Test)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "#model.compile(loss='binary_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "history = model.fit(x_Train,y_Train,batch_size=16,epochs=200, validation_data =(x_Test,y_Test))\n",
        "#print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtWP9RGGflIE"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQq3sWvQDbsZ"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G3EEiaBA5k-"
      },
      "source": [
        "#Alternate model for Valence - CNN\n",
        "\n",
        "#x_Train = x_Train.reshape(x_Train.shape[0],1,1)\n",
        "#x_Test = x_Test.reshape(x_Test.shape[0],1,1)\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(tf.keras.layers.Flatten())\n",
        "model.add(Dense(128,activation =tf.nn.relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation =tf.nn.relu))\n",
        "model.add(Dense(10,activation =tf.nn.softmax))\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt ,metrics=['accuracy'])\n",
        "history_CNN = model.fit(x_Train,y_Train,epochs=100, validation_data =(x_Test,y_Test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr4KO1o5GFcz"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iet2OXe_zYiL"
      },
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.show()\n",
        " plt.plot(history_CNN.history['loss'])\n",
        " plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dio3bwThAmNN"
      },
      "source": [
        "#DATA PREP FOR AROUSAL\n",
        "\n",
        "#Arousal is judged by alpha beta ratio of AF3 and AF4\n",
        "#AF3 is data 1, AF4 is data 14\n",
        "df3=data2.reset_index()['psdalpha_1']\n",
        "df4=data2.reset_index()['psdbeta_1']\n",
        "\n",
        "df34 = df4/df3\n",
        "\n",
        "df5=data2.reset_index()['psdalpha_14']\n",
        "df6=data2.reset_index()['psdbeta_14']\n",
        "\n",
        "df56  = df5/df6\n",
        "\n",
        "df7 = df34.append(df56)\n",
        "df7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-BHZvhEBbQ7"
      },
      "source": [
        "target2 =data2['arousalhigh']\n",
        "#target2 = target2.append(target2)\n",
        "target2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXGiNKmB4YN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#df7 = np.array(df7, dtype = float)\n",
        "target2 = np.array(target2, dtype = float)\n",
        "x_Train2, x_Test2, y_Train2, y_Test2 = train_test_split(dfz, target2, test_size = 0.35, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsPF5WXmaoKo"
      },
      "source": [
        "x_Train2 = x_Train2.reshape(x_Train2.shape[0],1,42)\n",
        "x_Test2 = x_Test2.reshape(x_Test2.shape[0],1,42)\n",
        "x_Test2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-55ywpyCEEV"
      },
      "source": [
        "#model for Arousal\n",
        "\n",
        "#x_Train2 = x_Train2.reshape(x_Train2.shape[0],1,1)\n",
        "#x_Test2 = x_Test2.reshape(x_Test2.shape[0],1,1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128,activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128,activation ='relu'))\n",
        "model.add(Dense(32,activation ='relu'))\n",
        "model.add(Dense(10,activation ='softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "history = model.fit(x_Train2,y_Train2,epochs=1000, validation_data =(x_Test2,y_Test2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buc3NgChDNXI"
      },
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT9V5X3eG1FT"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deXLF_lVMzHg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}